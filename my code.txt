# ------------------------------------------------------------------------------------------------------------------------------
# Joseph's code

q_table = [[0, 0, 0, 0], [0, 0, 0, 0]]

gamma = 0.8

# reward = -100

visited = []


def computeReward(state):
    if state == goal_state:
        return 100

    elif state in visited:
        return -10

    else:
        return 10


def getNextStates(state):  # returns the q values of next possible states
    stateLeft = (state[0] - 1, state[1])
    stateRight = (state[0] + 1, state[1])
    stateUp = (state[0], state[1] + 1)
    stateDown = (state[0], state[1] - 1)

    q0 = computeReward(stateLeft) + gamma * max(
        q_table[15 * stateLeft[0] + stateLeft[1]][0],
        q_table[15 * stateLeft[0] + stateLeft[1]][1],
        q_table[15 * stateLeft[0] + stateLeft[1]][2],
        q_table[15 * stateLeft[0] + stateLeft[1]][3],
    )
    q1 = computeReward(stateUp) + gamma * max(
        q_table[15 * stateUp[0] + stateUp[1]][0],
        q_table[15 * stateUp[0] + stateUp[1]][1],
        q_table[15 * stateUp[0] + stateUp[1]][2],
        q_table[15 * stateUp[0] + stateUp[1]][3],
    )
    q2 = computeReward(stateRight) + gamma * max(
        q_table[15 * stateRight[0] + stateRight[1]][0],
        q_table[15 * stateRight[0] + stateRight[1]][1],
        q_table[15 * stateRight[0] + stateRight[1]][2],
        q_table[15 * stateRight[0] + stateRight[1]][3],
    )
    q3 = computeReward(stateDown) + gamma * max(
        q_table[15 * stateDown[0] + stateDown[1]][0],
        q_table[15 * stateDown[0] + stateDown[1]][1],
        q_table[15 * stateDown[0] + stateDown[1]][2],
        q_table[15 * stateDown[0] + stateDown[1]][3],
    )

    next_states = (q0, q1, q2, q3)

    return next_states


def computeQValue(state, action):
    # q_table = [[0,0,0,0],[0,0,0,0]] list of states and their actions inside them

    # q(state, action) = r(state, action) + gamma * Max[Q_nextState()]

    reward = computeReward(state)

    q = reward + gamma * max(getNextStates(state))

    q_table[state][action] = q


def q_training():
    run = True

    while run:
        # get state

        next_state = 0
        next_action = 0

        current_state = getState()

        Q_up = computeQValue(current_state, 0)
        Q_right = computeQValue(current_state, 1)
        Q_down = computeQValue(current_state, 2)
        Q_left = computeQValue(current_state, 3)

        best_Q = Q_up

        Q_options = (Q_right, Q_down, Q_left)

        for i in range(len(Q_options)):
            if Q_options[i] > best_Q:
                next_state = Q_options[i]
                next_action = i

        nextMove(next_state, next_action)